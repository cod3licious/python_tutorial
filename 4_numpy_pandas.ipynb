{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Science Libraries\n",
    "\n",
    "This notebook contains a brief introduction into the basic data science libraries `numpy`, `pandas`, and `matplotlib`. These libraries help you to efficiently load, store, manipulate, and look at your data. It is especially important that you know how to access individual elements/rows/columns from a matrix or table with various indexing techniques. Therefore, make sure to play around here a bit until you feel comfortable with these methods.\n",
    "\n",
    "\n",
    "**Exercise:** After you're done with the tutorial, find a dataset of your own, load it with pandas, and examine the different variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy\n",
    "The `numpy` library is used for mathematical operations and scientific computations. If you need more advanced operations, also check out the `scipy` library, which is closely related to numpy. If you have worked with MATLAB before, many things here will seem very familiar, just always remember that in Python you start indexing at 0, not 1.\n",
    "\n",
    "The most important data structure in numpy is the so called `array`, which is used to represent a vector or matrix.\n",
    "\n",
    "**Official `numpy` tutorial:** https://numpy.org/devdocs/user/quickstart.html\n",
    "\n",
    "(If you're interested in more advanced scientific programming (e.g. optimization), you may also want to check out the official [scipy tutorial](https://docs.scipy.org/doc/scipy/reference/tutorial/index.html) for more information on the `scipy` library.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the library with its standard abbreviation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vector with 3 dimensions from a list\n",
    "a = np.array([1., -2., 0.])\n",
    "# look at the vector\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 2x3 matrix from nested lists\n",
    "M = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply all elements in the matrix by 3\n",
    "3*M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply the matrix M with the vector a\n",
    "np.dot(M, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply the matrix M with its transpose\n",
    "np.dot(M, M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elementwise multiplication\n",
    "M*M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the dimensions always line up, otherwise you'll get an error like this\n",
    "np.dot(M, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of a matrix or vector (e.g. to investigate errors like the one above)\n",
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3 dimensional identity matrix\n",
    "np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 3x2 matrix with zeros\n",
    "np.zeros((3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random provides different options to create random data.\n",
    "# here we create a 4x4 matrix with random, normally distributed values.\n",
    "# you might want to set a random seed first to get reproducible results:\n",
    "# --> execute the cell a few times to see you always get a different matrix\n",
    "# --> then uncomment the line below and excecute it again a few times\n",
    "# np.random.seed(13)\n",
    "R = np.random.randn(4, 4)\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexing of matrices works similar to indexing lists\n",
    "# remember: indexing starts at 0 and the last element is exclusive\n",
    "# this gives you the first 2 rows with all columns\n",
    "R[:2, :]  # for rows, you can also ommit the last :, i.e., write R[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all rows starting at the 3rd row with all columns\n",
    "R[2:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 2 and 4\n",
    "R[:, [1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 3 - notice the shape of the returned array, \n",
    "# i.e., it's a proper column vector (shape: (4, 1))\n",
    "R[:, [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column 3 but as a flattened array (shape: (4,))\n",
    "R[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a binary mask that indicates which values in R are smaller than 0\n",
    "M = (R < 0)\n",
    "M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set all entries in R that are smaller than 0 to -99\n",
    "R[M] = -99.\n",
    "R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "\n",
    "With the `matplotlib` library, it is possible to create highly customizable plots. It is also the basis for more advanced plotting libraries such as `seaborn`.\n",
    "\n",
    "[This set of cheat sheets](https://github.com/matplotlib/cheatsheets) may be helpful for creating the perfect plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import with standard abbreviation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some data that we want to plot\n",
    "x = np.arange(10)  # numpy array with numbers 0 - 9\n",
    "y = x**2           # squared numbers\n",
    "# create a very basic plot of x vs. y\n",
    "plt.figure()   # new canvas\n",
    "plt.plot(x, y) # simple line plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more advanced plot with axis labels etc.\n",
    "plt.figure()\n",
    "plt.plot(x, y, label=\"x^2\")  # 'label' is later used in the legend\n",
    "plt.plot(x, x**3, \"r\", label=\"x^3\") # \"r\" creates a red line\n",
    "# axis labels, legend based on the specified labels, and title\n",
    "plt.xlabel(\"X axis\")\n",
    "plt.ylabel(\"Y axis\")\n",
    "plt.legend(loc=0)  # loc=0 automatically determines the best location for the legend\n",
    "plt.title(\"Title of the figure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create randomly distributed data\n",
    "x = np.random.randn(100)\n",
    "y = np.random.randn(100)\n",
    "# create a scatter plot of x vs y\n",
    "plt.figure()\n",
    "# by passing for c (=color) an array of the same length as x and y,\n",
    "# each dot can have its individual color\n",
    "plt.scatter(x, y, c=x)\n",
    "plt.xlabel(\"X axis\")\n",
    "plt.ylabel(\"Y axis\")\n",
    "plt.colorbar()  # creates a colorbar (more appropriate than a legend here)\n",
    "plt.title(\"Title of the figure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas\n",
    "\n",
    "The `pandas` library takes our basic data manipulation to the next level. It is a lifesaver if you need to read in nasty excel files and helps you with all your basic data science tasks, e.g., if you want to get a quick overview of your data or create some simple plots. If you have used the R programming language before, some concepts here might be familiar to you.\n",
    "\n",
    "The most important data structure in pandas is the `DataFrame`, which is simply a table with all your data in it. \n",
    "We'll always assume that our data is structured such that each row corresponds to one data point (or observation), while each column represents a different attribute/variable that was measured for the data points (in machine learning contexts, these different attributes are usually refered to as \"features\"). \n",
    "\n",
    "**Official `pandas` tutorial:** https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import with standard abbreviation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe from a numpy array M with 10 rows and 3 columns\n",
    "M = np.random.random((10, 3))\n",
    "df = pd.DataFrame(M)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe back to a numpy array\n",
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a more interesting dataframe from a dictionary (keys are columns (=\"features\"))\n",
    "df = pd.DataFrame(\n",
    "       {\n",
    "          'sex': ['m', 'w', 'm', 'w'],\n",
    "          'height': [1.80, 1.77, 1.89, 1.65],\n",
    "          'weight': [65.3, 73.4, 80.0, 77.0],\n",
    "          'subject_id': ['subject1', 'subject8', 'subject12', 'subject23']\n",
    "       }\n",
    ")\n",
    "# look at the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the additional column on the left with 0-3 above; this is the index column.\n",
    "# for easier handeling of the data, we can explicitly set the subject_id column as our index\n",
    "df = df.set_index('subject_id')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic manipulations & statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the column \"sex\" from the dataframe \n",
    "# (returns a pandas Series, similar to a flat array in numpy)\n",
    "df['sex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by selecting a list of columns, the DataFrame structure is preserved\n",
    "df[['sex']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column (similar as adding a new key-value pair to a dict)\n",
    "# compute with other columns from the dataframe\n",
    "df['BMI'] = df['weight'] / (df['height'] ** 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute basic statistics on the dataframe\n",
    "df['BMI'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics for the whole dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also compute different aggregations for different columns; just pass\n",
    "# any function that is then called on the specified column, e.g. np.mean(df[\"BMI\"])\n",
    "# maximum for height, minimum for weight, mean for BMI\n",
    "df.agg({'height': max, 'weight': min, 'BMI': np.mean}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by grouping based on one column...\n",
    "g = df.groupby('sex')\n",
    "# ...we can compute statistics for different groups\n",
    "g[\"BMI\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregations work here too\n",
    "g.agg({'height': max, 'weight': min, 'BMI': np.mean}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import & Export\n",
    "\n",
    "Have a look at the [pandas tutorial](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) for more info on possible file formats as well as additional options for saving and loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can export our data as a .csv file (other formats are also supported)\n",
    "# (--> have a look at the folder to see the file that was created!)\n",
    "df.to_csv('bmi_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also read in files and create a dataframe from them\n",
    "df_imported = pd.read_csv('bmi_dataset.csv')\n",
    "df_imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how our index column was treated like just a regular column.\n",
    "# with additional options, we can already correctly set our index column while loading.\n",
    "# other options also allow to e.g. skip some lines at the beginning of a file, etc.\n",
    "df_imported = pd.read_csv('bmi_dataset.csv', index_col='subject_id')\n",
    "df_imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas infers the data type of the columns when reading in the data \n",
    "df_imported.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a view of the dataframe with a binary mask based on the column \"sex\"\n",
    "df[df['sex'] == 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only entries from the column \"height\" with the binary mask\n",
    "df['height'][df['sex'] == 'm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe based on the height\n",
    "df[df['height'] < 1.80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a specific data point based on the index name\n",
    "df.loc['subject12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a specific data point based on the row number\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc works similarly to numpy indexing\n",
    "df.iloc[:2, [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a specific entry in the dataframe using index name and column name\n",
    "df.loc['subject8', 'BMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also works with lists of names\n",
    "df.loc[['subject8', 'subject12'], ['BMI', 'sex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with missing values (NaNs)\n",
    "NaN (\"Not a Number\") or missing values in your data are no fun and can lead to errors when using this data down the road. Therefore, you should remove these entries (or fill them with sensible defaults, though this can result in other problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in a dataset\n",
    "df = pd.read_csv(\"data/test_na.csv\")\n",
    "# there are some NaNs!\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many NaNs you have per column\n",
    "# (--> try also without the .sum() or .sum(axis=1))\n",
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you could fill the missing values with some defaults, e.g. 0\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or with mean values - use with EXTREME caution!!!\n",
    "df.fillna(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# usually it's safest if we just remove the data points that have NaNs anywhere\n",
    "# instead of adding garbage values to the data.\n",
    "# but be aware that NaNs might not be random, e.g., in surveys rich people might more\n",
    "# often decline to answer questions about their wealth than middle class people\n",
    "# so removing data points this way can create a systematic bias in the dataset.\n",
    "# what ever you do, just be sure to note your decision somewhere\n",
    "# and communicate it when you present your results!\n",
    "df = df.dropna(axis=0, how=\"any\")  # axis=0: drop rows, not columns; how='any': drop if there is a NaN in one field\n",
    "# rows 1 and 3 are missing now\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since some rows were removed, be careful when indexing the df!\n",
    "# row 0 is still present\n",
    "print(\"row 0:\\n\", df.iloc[0])\n",
    "# row 0 also has index 0\n",
    "print(\"\\n\\nindex 0:\\n\", df.loc[0])\n",
    "# row 1 now corresponds to index 2\n",
    "print(\"\\n\\nrow 1:\\n\", df.iloc[1])\n",
    "print(\"\\n\\nindex 2:\\n\", df.loc[2])\n",
    "# index 1 is missing - this will give a KeyError!\n",
    "print(\"\\n\\nindex 1:\\n\", df.loc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid errors, it's often a good idea to reset the index\n",
    "# inplace: change df directly instead of returning a new object\n",
    "# drop: don't keep old index\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "# df.loc[1] would work again now\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining and transforming features\n",
    "In machine learning you usually want your variables to be normally distributed. When you get a new dataset, always plot all the variables and if they aren't (approximately) normally/uniformly distributed, consider applying a transformation, e.g., take the logarithm of a variable with a long tail of extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of values for each column\n",
    "df.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot x2 again to examine these few large values in a bit more detail\n",
    "plt.figure()\n",
    "# plot x2 against some small random jitter so the dots don't overlap too much\n",
    "plt.scatter(df[\"x2\"], 0.1*np.random.randn(len(df[\"x2\"])))\n",
    "plt.xlabel(\"x2\")\n",
    "plt.ylabel(\"random jitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a transformation of x2 to get the values more normally distributed, i.e.,\n",
    "# without these few very extreme values\n",
    "x2_new = np.log(df[\"x2\"] + 1)\n",
    "plt.figure()\n",
    "# not perfect, but much better!\n",
    "plt.scatter(x2_new, 0.1*np.random.randn(len(x2_new)))\n",
    "plt.xlabel(\"x2\")\n",
    "plt.ylabel(\"random jitter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with time series data\n",
    "Time series data is often stored in specialized databases, which sometimes export the data in a so-called \"long format\", where the variables are no longer stored in individual columns (like in the regular \"wide format\" that we were dealing with so far). To later use this data with our machine learning methods, we therefore first need to transform it into the wide format (data points/observations in rows, variables/features in columns).\n",
    "\n",
    "The dataset is originally from [here](http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection+) but was modified for our purposes. It contains timestamped data of different sensors (temp, co2, etc.) in a room and how many people are currently present in the room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a time series dataset in long format\n",
    "df = pd.read_csv(\"data/test_timeseries.csv\")\n",
    "# variable contains the column name and value the corresponding value at this timestamp\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what kind of variables there are --> these will be our columns later\n",
    "df[\"variable\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what data types were detected\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apparently, timestamp was detected as a string (\"object\").\n",
    "# we want to transform it into a proper datetime format to enable\n",
    "# a bunch of data operations on time stamps (incl. correct plotting)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we want to convert the long format to our regular wide format\n",
    "df_wide = df.pivot(index='timestamp', columns='variable')\n",
    "# by default, the columns are now a MultiIndex; with droplevel we can transform them \n",
    "# into a regular Index so we can work with the dataframe as we're used to\n",
    "df_wide.columns = df_wide.columns.droplevel(0)\n",
    "df_wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the time series (this only works so nicely because \n",
    "# the timestamp is the index column and was correctly formatted!)\n",
    "df_wide.plot(subplots=True);  # the \";\" at the end prevents unnecessary additional output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# often, time series data is sampled with a very high frequency (every few seconds)\n",
    "# but the sensor measurements might not change that much over time\n",
    "# you can use \"resample\" to change the sampling frequency and reduce the size of your dataset.\n",
    "# by calling .mean() on the resampler, we tell it to compute the new values as the mean\n",
    "# of the values in one interval. you could also use the function to upsample your data.\n",
    "print(\"original shape:\", df_wide.shape)\n",
    "df_wide = df_wide.resample(\"10min\").mean()\n",
    "print(\"downsampled shape:\", df_wide.shape)\n",
    "# looks a bit smoother than above\n",
    "df_wide.plot(subplots=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Work with your own data\n",
    "\n",
    "Find a dataset, load it with pandas, make sure it is in the correct (wide) format (data points in rows, variables/features in columns; columns should have the correct column names), and examine the different variables (e.g. by plotting them) to see if they are (approximately) normally/uniformly distributed.\n",
    "\n",
    "You can also do this in a new notebook, just remember to import all needed libraries.\n",
    "\n",
    "If you don't have a dataset of your own that you want to explore, maybe you'll find one here that interests you: https://www.kaggle.com/datasets?fileType=csv&sizeEnd=100%2CMB\n",
    "\n",
    "If you're not working on your own computer, but e.g. online with Binder you can go back to the folder view with the list of notebooks and in the top right there is a button \"Upload\" where you can e.g. upload a .csv file from your computer to work with online.\n",
    "\n",
    "Don't be afraid to google if you don't know how to do something (tip: search in English to get more results!). You can also get the documentation of any function by writing a `?` after its name to see, for example, what parameters you can pass to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute this to see the documentation of the read_csv function\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
